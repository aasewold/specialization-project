\chapter{Methodology}
\label{chap:method}


% The method chapter should describe in detail which activities you undertake to answer the research questions presented in the introduction, and why they were chosen. This includes detailed descriptions of experiments, surveys, computations, data analysis, statistical tests etc.

\section{Tools and Resources}
Beskrive VM, IDUN, etc


\section{Creating Python Stubs for CARLA (?)}

passer under contributions?


\section{Upgrading TransFuser to CARLA Version 0.9.13}

\section{Experiments}

In this paper we perform two experiments.
First, we establish a quantitative performance baseline 
by evaluating the published pre-trained agent on the \textit{Longest6} benchmark.
Then, using the original dataset, we train the TransFuser model from scratch,
and evaluate it using the same benchmark.
Using the same environment for both of these experiments ensures that the results are comparable,
but for curiosity we also compare the results with the published results in \cite{transfuser-pami}.

We also perform qualitative analysis of the results by visually comparing the driving performance
with the published videos\footnote{\url{https://www.youtube.com/playlist?list=PL6LvknlY2HlQG3YQ2nMIx7WcnyzgK9meO}}
showcasing the original TransFuser's behaviour on this benchmark.


\subsection{Experiment 1: Evaluating TransFuser with pre-trained parameters}
\label{sec:method:experiment1}

We evaluate TransFuser on the Longest6 benchmark following the evaluation instructions at\footnote{
\url{https://github.com/autonomousvision/transfuser/\#evaluation}}.
Specifically,
we use Carla version 0.9.13 and our TransFuser code adapted to this version,
and run the evaluation program using the pre-trained parameters available at\footnote{
\url{https://github.com/autonomousvision/transfuser/\#pretrained-agents}}.
Carla is run headlessly inside a Docker container,
and inside another Docker container we evaluate TransFuser
by setting the environment variable \texttt{TEAM\_AGENT} to \texttt{team\_code\_transfuser/submission\_agent.py}
and running the script \texttt{leaderboard/scripts/local\_evaluation.sh}.

The results are then processed using the script \texttt{tools/result\_parser.py},


\subsection{Experiment 2: Evaluating TransFuser with re-trained parameters}

To get familiar with the training procedure and requirements,
we perform one full training run using the TransFuser dataset and randomly initialized weights.
We download the original dataset and prepare a Docker container with the required dependencies.
Then, we run the provided script \texttt{team\_code\_transfuser/train.py} to train our own TransFuser.
The model is trained with the same parameters as specified in \cite{transfuser-pami}.
Using two Nvidia A100 GPUs, this translates to approximately 40 hours.

Using the trained model weights,
we then apply the same procedure as in section \ref{sec:method:experiment1}
to evaluate the agent's performance.

As pointed out in \cite{transfuser-pami},
agent performance when training with imitation learning
can be prone to initial weight initialization.
We therefore, like the TransFuser authors,
train multiple models with different seeds to measure variance in the performance,
and also evaluate the performance of an ensemble of these different models.
